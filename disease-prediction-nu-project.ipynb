{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split,KFold,cross_val_score,GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix,classification_report,plot_confusion_matrix,plot_roc_curve,precision_score,roc_curve\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom pandas_profiling import ProfileReport\nfrom sklearn.linear_model import LogisticRegression, Perceptron, RidgeClassifier, SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier \nfrom sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, VotingClassifier \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":1.394335,"end_time":"2021-08-30T15:05:17.551091","exception":false,"start_time":"2021-08-30T15:05:16.156756","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd /kaggle/input/encoding","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd /kaggle/input/disease-symptom-description-dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Read and shuffle the dataset**","metadata":{"papermill":{"duration":0.024723,"end_time":"2021-08-30T15:05:17.748249","exception":false,"start_time":"2021-08-30T15:05:17.723526","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df = pd.read_csv('dataset.csv')\ndf = shuffle(df,random_state=42)\ndf.head()","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","papermill":{"duration":0.098404,"end_time":"2021-08-30T15:05:17.923833","exception":false,"start_time":"2021-08-30T15:05:17.825429","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Removing Hyphen from strings**","metadata":{}},{"cell_type":"code","source":"for col in df.columns:\n    \n    df[col] = df[col].str.replace('_',' ')\ndf.head()    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dataset characteristics**","metadata":{"papermill":{"duration":0.030266,"end_time":"2021-08-30T15:05:17.979853","exception":false,"start_time":"2021-08-30T15:05:17.949587","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df.describe()","metadata":{"papermill":{"duration":0.139299,"end_time":"2021-08-30T15:05:18.145989","exception":false,"start_time":"2021-08-30T15:05:18.00669","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check for null and NaN values**","metadata":{"papermill":{"duration":0.025542,"end_time":"2021-08-30T15:05:18.197707","exception":false,"start_time":"2021-08-30T15:05:18.172165","status":"completed"},"tags":[]}},{"cell_type":"code","source":"null_checker = df.apply(lambda x: sum(x.isnull())).to_frame(name='count')\nprint(null_checker)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.plot(null_checker.index, null_checker['count'])\nplt.xticks(null_checker.index, null_checker.index, rotation=45,\nhorizontalalignment='right')\nplt.title('Before removing Null values')\nplt.xlabel('column names')\nplt.margins(0.1)\nplt.show()","metadata":{"papermill":{"duration":0.044062,"end_time":"2021-08-30T15:05:18.267712","exception":false,"start_time":"2021-08-30T15:05:18.22365","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Remove the trailing space from the symptom columns**","metadata":{"papermill":{"duration":0.026579,"end_time":"2021-08-30T15:05:18.393419","exception":false,"start_time":"2021-08-30T15:05:18.36684","status":"completed"},"tags":[]}},{"cell_type":"code","source":"cols = df.columns\ndata = df[cols].values.flatten()\n\ns = pd.Series(data)\ns = s.str.strip()\ns = s.values.reshape(df.shape)\n\ndf = pd.DataFrame(s, columns=df.columns)\ndf.head()","metadata":{"papermill":{"duration":0.097172,"end_time":"2021-08-30T15:05:18.517252","exception":false,"start_time":"2021-08-30T15:05:18.42008","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Fill the NaN values with zero**","metadata":{"papermill":{"duration":0.027747,"end_time":"2021-08-30T15:05:18.57314","exception":false,"start_time":"2021-08-30T15:05:18.545393","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df = df.fillna(0)\ndf.head()","metadata":{"papermill":{"duration":0.066576,"end_time":"2021-08-30T15:05:18.667519","exception":false,"start_time":"2021-08-30T15:05:18.600943","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Symptom severity rank**","metadata":{"papermill":{"duration":0.027881,"end_time":"2021-08-30T15:05:18.7235","exception":false,"start_time":"2021-08-30T15:05:18.695619","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df1 = pd.read_csv('Symptom-severity.csv')\ndf1['Symptom'] = df1['Symptom'].str.replace('_',' ')\ndf1.head()","metadata":{"papermill":{"duration":0.04616,"end_time":"2021-08-30T15:05:18.797645","exception":false,"start_time":"2021-08-30T15:05:18.751485","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Get overall list of symptoms**","metadata":{"papermill":{"duration":0.028392,"end_time":"2021-08-30T15:05:18.854505","exception":false,"start_time":"2021-08-30T15:05:18.826113","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df1['Symptom'].unique()","metadata":{"papermill":{"duration":0.039041,"end_time":"2021-08-30T15:05:18.92221","exception":false,"start_time":"2021-08-30T15:05:18.883169","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Encode symptoms in the data with the symptom rank**","metadata":{"papermill":{"duration":0.029149,"end_time":"2021-08-30T15:05:18.980116","exception":false,"start_time":"2021-08-30T15:05:18.950967","status":"completed"},"tags":[]}},{"cell_type":"code","source":"vals = df.values\nsymptoms = df1['Symptom'].unique()\n\nfor i in range(len(symptoms)):\n    vals[vals == symptoms[i]] = df1[df1['Symptom'] == symptoms[i]]['weight'].values[0]\n    \nd = pd.DataFrame(vals, columns=cols)\nd.head()","metadata":{"papermill":{"duration":0.689792,"end_time":"2021-08-30T15:05:19.699129","exception":false,"start_time":"2021-08-30T15:05:19.009337","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Assign symptoms with no rank to zero**","metadata":{"papermill":{"duration":0.029325,"end_time":"2021-08-30T15:05:19.757938","exception":false,"start_time":"2021-08-30T15:05:19.728613","status":"completed"},"tags":[]}},{"cell_type":"code","source":"d = d.replace('dischromic  patches', 0)\nd = d.replace('spotting  urination',0)\ndf = d.replace('foul smell of urine',0)\ndf.head(10)","metadata":{"papermill":{"duration":0.100573,"end_time":"2021-08-30T15:05:19.888131","exception":false,"start_time":"2021-08-30T15:05:19.787558","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check if entire columns have zero values so we can drop those values**","metadata":{"papermill":{"duration":0.02973,"end_time":"2021-08-30T15:05:19.948081","exception":false,"start_time":"2021-08-30T15:05:19.918351","status":"completed"},"tags":[]}},{"cell_type":"code","source":"null_checker = df.apply(lambda x: sum(x.isnull())).to_frame(name='count')\nprint(null_checker)","metadata":{"papermill":{"duration":0.046686,"end_time":"2021-08-30T15:05:20.024958","exception":false,"start_time":"2021-08-30T15:05:19.978272","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.plot(null_checker.index, null_checker['count'])\nplt.xticks(null_checker.index, null_checker.index, rotation=45,\nhorizontalalignment='right')\nplt.title('After removing Null values')\nplt.xlabel('column names')\nplt.margins(0.01)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of symptoms used to identify the disease \",len(df1['Symptom'].unique()))\nprint(\"Number of diseases that can be identified \",len(df['Disease'].unique()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compare linear relationships between attributes using correlation coefficient","metadata":{}},{"cell_type":"code","source":"new_df = pd.read_csv('/kaggle/input/encoding/newdf.csv')\nencoding = pd.read_csv('/kaggle/input/encoding/decryption.csv',index_col=0)\nnew_df.drop(labels=[\"132\",'74','0'],axis=1,inplace=True)\nrenameddf=new_df.rename(errors='raise',inplace=False,columns= {str(x):y for y,x in encoding.to_dict()['code'].items()})\nrenameddf\nplt.figure(figsize= (15,10))\nsns.heatmap(renameddf.corr(),cmap=\"PuBu\",annot=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# pandas report on dataset","metadata":{}},{"cell_type":"code","source":"# ProfileReport(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Get the names of diseases from data**","metadata":{"papermill":{"duration":0.031307,"end_time":"2021-08-30T15:05:20.164109","exception":false,"start_time":"2021-08-30T15:05:20.132802","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df['Disease'].unique()","metadata":{"papermill":{"duration":0.041096,"end_time":"2021-08-30T15:05:20.236599","exception":false,"start_time":"2021-08-30T15:05:20.195503","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Select the features as symptoms column and label as Disease column\n\nExplination: A **feature** is an input; **label** is an output.\nA feature is one column of the data in your input set. For instance, if you're trying to predict the type of pet someone will choose, your input features might include age, home region, family income, etc. The label is the final choice, such as dog, fish, iguana, rock, etc.\n\nOnce you've trained your model, you will give it sets of new input containing those features; it will return the predicted \"label\" (pet type) for that person.","metadata":{"papermill":{"duration":0.031128,"end_time":"2021-08-30T15:05:20.299065","exception":false,"start_time":"2021-08-30T15:05:20.267937","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data = df.iloc[:,1:].values\nlabels = df['Disease'].values","metadata":{"papermill":{"duration":0.040479,"end_time":"2021-08-30T15:05:20.434812","exception":false,"start_time":"2021-08-30T15:05:20.394333","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting the dataset to training (80%) and testing (20%)\n\nSeparating data into training and testing sets is an important part of evaluating data mining models. Typically, when you separate a data set into a training set and testing set, most of the data is used for training, and a smaller portion of the data is used for testing. By using similar data for training and testing, you can minimize the effects of data discrepancies and better understand the characteristics of the model.\nAfter a model has been processed by using the training set, we test the model by making predictions against the test set. Because the data in the testing set already contains known values for the attribute that you want to predict, it is easy to determine whether the model's guesses are correct.\n\n* Train Dataset: Used to fit the machine learning model.\n* Test Dataset: Used to evaluate the fit machine learning model.","metadata":{"papermill":{"duration":0.030902,"end_time":"2021-08-30T15:05:20.497131","exception":false,"start_time":"2021-08-30T15:05:20.466229","status":"completed"},"tags":[]}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(data, labels, train_size = 0.8,random_state=42)\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n**Initialize and train a Support vector classifier**","metadata":{"papermill":{"duration":0.03164,"end_time":"2021-08-30T15:05:20.636211","exception":false,"start_time":"2021-08-30T15:05:20.604571","status":"completed"},"tags":[]}},{"cell_type":"code","source":"SVM_unhyperd= SVC()\nSVM_unhyperd.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compute the F1 score, also known as balanced F-score or F-measure.\n\nThe F1 score can be interpreted as a weighted average of the precision and\nrecall, where an F1 score reaches its best value at 1 and worst score at 0.\nThe relative contribution of precision and recall to the F1 score are\nequal. The formula for the F1 score is\n\n    F1 = 2 * (precision * recall) / (precision + recall)","metadata":{}},{"cell_type":"code","source":"preds = SVM_unhyperd.predict(x_test)\nconf_mat = confusion_matrix(y_test, preds)\ndf_cm = pd.DataFrame(conf_mat, index=df['Disease'].unique(), columns=df['Disease'].unique())\nprint('F1-score% =', f1_score(y_test, preds, average='macro')*100, '|', 'Accuracy% =', accuracy_score(y_test, preds)*100,'|', 'Precision% =', precision_score(y_test, preds,average='macro')*100)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plot the confusion matrix for 25 diseases**","metadata":{}},{"cell_type":"code","source":"sns.heatmap(df_cm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = KFold(n_splits=10,shuffle=True,random_state=42)\nSVM_unhyperd_train =cross_val_score(SVM_unhyperd, x_train, y_train, cv=kfold, scoring='accuracy')\npd.DataFrame(SVM_unhyperd_train,columns=['Scores'])\nprint(\"Mean Accuracy: %.3f%%, Standard Deviation: (%.2f%%)\" % (SVM_unhyperd_train.mean()*100.0, SVM_unhyperd_train.std()*100.0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = KFold(n_splits=10,shuffle=True,random_state=42)\nSVM_unhyperd_test =cross_val_score(SVM_unhyperd, x_test, y_test, cv=kfold, scoring='accuracy')\npd.DataFrame(SVM_unhyperd_test,columns=['Scores'])\nprint(\"Mean Accuracy: %.3f%%, Standard Deviation: (%.2f%%)\" % (SVM_unhyperd_test.mean()*100.0, SVM_unhyperd_test.std()*100.0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter tuning with GridSearchCV\n\nPerforming hyperparameter tuning in order to determine the optimal values for our given model.The performance of a model significantly depends on the value of hyperparameters. There is no way to know in advance the best values for hyperparameters so ideally, we need to try all possible values to know the optimal values. Doing this manually could take a considerable amount of time and resources and thus we used GridSearchCV to automate the tuning of hyperparameters.\n**Note:** The ouput of the Gridsearchcv is **SVC(C=0.02, gamma=0.3, kernel='poly')**","metadata":{}},{"cell_type":"code","source":"# param_grid = {'C': [0.2,0.4,0.6], 'gamma': [0.2,0.3,0.4,0],'kernel': ['linear','poly', 'sigmoid']}\n# grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\n# grid.fit(x_train,y_train)\n# print(grid.best_estimator_)\n# grid_predictions = grid.predict(x_test)\n# print(confusion_matrix(y_test,grid_predictions))\n# print(classification_report(y_test,grid_predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVM_hyperd = SVC(C=0.02, gamma=0.3, kernel='poly')\nSVM_hyperd.fit(x_train, y_train)\npreds = SVM_hyperd.predict(x_test)\nconf_mat = confusion_matrix(y_test, preds)\ndf_cm = pd.DataFrame(conf_mat, index=df['Disease'].unique(), columns=df['Disease'].unique())\nprint('F1-score% =', f1_score(y_test, preds, average='macro')*100, '|', 'Accuracy% =', accuracy_score(y_test, preds)*100)\nsns.heatmap(df_cm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using 10-Fold Cross Validation to estimate the performance of machine learning models\n\nThe procedure provides an estimate of the model performance on the dataset when making a prediction on data not used during training. It is less biased than some other techniques, such as a single train-test split for small- to modestly-sized dataset","metadata":{}},{"cell_type":"code","source":"kfold = KFold(n_splits=10,shuffle=True,random_state=42)\nSVM_hyperd_train =cross_val_score(SVM_hyperd, x_train, y_train, cv=kfold, scoring='accuracy')\npd.DataFrame(SVM_hyperd_train,columns=['Scores'])\nprint(\"Mean Accuracy: %.3f%%, Standard Deviation: (%.2f%%)\" % (SVM_hyperd_train.mean()*100.0, SVM_hyperd_train.std()*100.0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = KFold(n_splits=10,shuffle=True,random_state=42)\nSVM_hyperd_test =cross_val_score(SVM_hyperd, x_test, y_test, cv=kfold, scoring='accuracy')\npd.DataFrame(SVM_hyperd_test,columns=['Scores'])\nprint(\"Mean Accuracy: %.3f%%, Standard Deviation: (%.2f%%)\" % (SVM_hyperd_test.mean()*100.0, SVM_hyperd_test.std()*100.0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Naive Bayes Model","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\npreds=gaussian.predict(x_test)\nconf_mat = confusion_matrix(y_test, preds)\ndf_cm = pd.DataFrame(conf_mat, index=df['Disease'].unique(), columns=df['Disease'].unique())\nprint('F1-score% =', f1_score(y_test, preds, average='macro')*100, '|', 'Accuracy% =', accuracy_score(y_test, preds)*100)\nsns.heatmap(df_cm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = KFold(n_splits=10,shuffle=True,random_state=42)\ngaussian_train =cross_val_score(gaussian, x_train, y_train, cv=kfold, scoring='accuracy')\npd.DataFrame(gaussian_train,columns=['Scores'])\nprint(\"Mean Accuracy: %.3f%%, Standard Deviation: (%.2f%%)\" % (gaussian_train.mean()*100.0, gaussian_train.std()*100.0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = KFold(n_splits=10,shuffle=True,random_state=42)\ngaussian_test =cross_val_score(gaussian, x_test, y_test, cv=kfold, scoring='accuracy')\npd.DataFrame(gaussian_test,columns=['Scores'])\nprint(\"Mean Accuracy: %.3f%%, Standard Deviation: (%.2f%%)\" % (gaussian_test.mean()*100.0, gaussian_test.std()*100.0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree","metadata":{}},{"cell_type":"code","source":"tree =DecisionTreeClassifier(criterion='gini',random_state=42,max_depth=13)\ntree.fit(x_train, y_train)\npreds=tree.predict(x_test)\nconf_mat = confusion_matrix(y_test, preds)\ndf_cm = pd.DataFrame(conf_mat, index=df['Disease'].unique(), columns=df['Disease'].unique())\nprint('F1-score% =', f1_score(y_test, preds, average='macro')*100, '|', 'Accuracy% =', accuracy_score(y_test, preds)*100)\nsns.heatmap(df_cm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = KFold(n_splits=10,shuffle=True,random_state=42)\nDS_train =cross_val_score(tree, x_train, y_train, cv=kfold, scoring='accuracy')\npd.DataFrame(DS_train,columns=['Scores'])\nprint(\"Mean Accuracy: %.3f%%, Standard Deviation: (%.2f%%)\" % (DS_train.mean()*100.0, DS_train.std()*100.0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = KFold(n_splits=10,shuffle=True,random_state=42)\nDS_test =cross_val_score(tree, x_test, y_test, cv=kfold, scoring='accuracy')\npd.DataFrame(DS_test,columns=['Scores'])\nprint(\"Mean Accuracy: %.3f%%, Standard Deviation: (%.2f%%)\" % (DS_test.mean()*100.0, DS_test.std()*100.0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"rfc=RandomForestClassifier(random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rnd_forest = RandomForestClassifier(random_state=42, max_features='sqrt', n_estimators= 500, max_depth=13)\nrnd_forest.fit(x_train,y_train)\npreds=rnd_forest.predict(x_test)\nconf_mat = confusion_matrix(y_test, preds)\ndf_cm = pd.DataFrame(conf_mat, index=df['Disease'].unique(), columns=df['Disease'].unique())\nprint('F1-score% =', f1_score(y_test, preds, average='macro')*100, '|', 'Accuracy% =', accuracy_score(y_test, preds)*100)\nsns.heatmap(df_cm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = KFold(n_splits=10,shuffle=True,random_state=42)\nrnd_forest_train =cross_val_score(rnd_forest, x_train, y_train, cv=kfold, scoring='accuracy')\npd.DataFrame(rnd_forest_train,columns=['Scores'])\nprint(\"Mean Accuracy: %.3f%%, Standard Deviation: (%.2f%%)\" % (rnd_forest_train.mean()*100.0, rnd_forest_train.std()*100.0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = KFold(n_splits=10,shuffle=True,random_state=42)\nrnd_forest_test =cross_val_score(rnd_forest, x_test, y_test, cv=kfold, scoring='accuracy')\npd.DataFrame(rnd_forest_test,columns=['Scores'])\nprint(\"Mean Accuracy: %.3f%%, Standard Deviation: (%.2f%%)\" % (rnd_forest_test.mean()*100.0, rnd_forest_test.std()*100.0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fucntion to manually test the models","metadata":{}},{"cell_type":"code","source":"def predd(S1,S2,S3,S4,S5,S6,S7,S8,S9,S10,S11,S12,S13,S14,S15,S16,S17,x):\n    psymptoms = [S1,S2,S3,S4,S5,S6,S7,S8,S9,S10,S11,S12,S13,S14,S15,S16,S17]\n    print(psymptoms)\n    a = np.array(df1[\"Symptom\"])\n    b = np.array(df1[\"weight\"])\n    for j in range(len(psymptoms)):\n        for k in range(len(a)):\n            if psymptoms[j]==a[k]:\n                psymptoms[j]=b[k]\n\n    psy = [psymptoms]\n\n    pred2 = x.predict(psy)\n    print(\"The prediction is\",pred2[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sympList=df1[\"Symptom\"].to_list()\npredd(sympList[7],sympList[5],sympList[2],sympList[80],0,0,0,0,0,0,0,0,0,0,0,0,0,rnd_forest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sympList=df1[\"Symptom\"].to_list()\npredd(sympList[8],sympList[1],sympList[2],sympList[80],0,0,0,0,0,0,0,0,0,0,0,0,0,SVM_hyperd)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sympList=df1[\"Symptom\"].to_list()\npredd(sympList[8],sympList[5],sympList[2],sympList[80],0,0,0,0,0,0,0,0,0,0,0,0,0,SVM_unhyperd)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparison between algorithms testing and training","metadata":{}},{"cell_type":"code","source":"n_groups = 5\nalgorithms = ('Naive Bayes','Unhyperd SVM', 'Hyperd SVM','Decision Tree', 'Random Forest')\ntrain_accuracy = (gaussian_train.mean()*100.0, \n                 SVM_unhyperd_train.mean()*100.0,\n                 SVM_hyperd_train.mean()*100.0,\n                 DS_train.mean()*100.0,\n                 rnd_forest_train.mean()*100.0,\n                 )\n\n\ntest_accuracy = (gaussian_test.mean()*100.0, \n                 SVM_unhyperd_test.mean()*100.0,\n                 SVM_hyperd_test.mean()*100.0,\n                 DS_test.mean()*100.0,\n                 rnd_forest_test.mean()*100.0\n                )\n\nStandard_Deviation=(gaussian_test.std()*100.0, \n                 SVM_unhyperd_test.std()*100.0,\n                 SVM_hyperd_test.std()*100.0,\n                 DS_test.std()*100.0,     \n                 rnd_forest_test.std()*100.0\n                 \n                   )\n\n# create plot\nfig, ax = plt.subplots(figsize=(15, 10))\nindex = np.arange(n_groups)\nbar_width = 0.3\nopacity = 1\nrects1 = plt.bar(index, train_accuracy, bar_width, alpha = opacity, color='Cornflowerblue', label='Train')\nrects2 = plt.bar(index + bar_width, test_accuracy, bar_width, alpha = opacity, color='Teal', label='Test')\nrects3 = plt.bar(index + bar_width, Standard_Deviation, bar_width, alpha = opacity, color='red', label='Standard Deviation')\nplt.xlabel('Algorithm') # x axis label\nplt.ylabel('Accuracy (%)') # y axis label\nplt.ylim(0, 115)\nplt.title('Comparison of Algorithm Accuracies') # plot title\nplt.xticks(index + bar_width * 0.5, algorithms) # x axis data labels\nplt.legend(loc = 'upper right') # show legend\nfor index, data in enumerate(train_accuracy):\n    plt.text(x = index - 0.035, y = data + 1, s = round(data, 2), fontdict = dict(fontsize = 8))\nfor index, data in enumerate(test_accuracy):\n    plt.text(x = index + 0.25, y = data + 1, s = round(data, 2), fontdict = dict(fontsize = 8))\nfor index, data in enumerate(Standard_Deviation):\n    plt.text(x = index + 0.25, y = data + 1, s = round(data, 2), fontdict = dict(fontsize = 8))    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}